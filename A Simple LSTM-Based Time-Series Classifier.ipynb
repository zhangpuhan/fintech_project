{"cells":[{"metadata":{},"cell_type":"markdown","source":"# A Simple LSTM-Based Time-Series Classifier (PyTorch)\n\nThe Recurrent Neural Network (RNN) architecutres show impressive results in tasks related to time-series processing and prediction. In this kernel, we're going to build a very simple LSTM-based classifier as an example of how one can apply RNN to classify a time-series data."},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from multiprocessing import cpu_count\nfrom pathlib import Path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.optim.lr_scheduler import _LRScheduler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 1\nnp.random.seed(seed)\ntorch.cuda.set_device(0)  # if you have more than one CUDA device","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading The Data\n\nFirst of all, we read the files and drop the irrelavant columns."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"ROOT = Path.cwd().parent/'input'\nSAMPLE = ROOT/'sample_submission.csv'\nTRAIN = ROOT/'X_train.csv'\nTARGET = ROOT/'y_train.csv'\nTEST = ROOT/'X_test.csv'\n\nID_COLS = ['series_id', 'measurement_number']\n\nx_cols = {\n    'series_id': np.uint32,\n    'measurement_number': np.uint32,\n    'orientation_X': np.float32,\n    'orientation_Y': np.float32,\n    'orientation_Z': np.float32,\n    'orientation_W': np.float32,\n    'angular_velocity_X': np.float32,\n    'angular_velocity_Y': np.float32,\n    'angular_velocity_Z': np.float32,\n    'linear_acceleration_X': np.float32,\n    'linear_acceleration_Y': np.float32,\n    'linear_acceleration_Z': np.float32\n}\n\ny_cols = {\n    'series_id': np.uint32,\n    'group_id': np.uint32,\n    'surface': str\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_trn = pd.read_csv(TRAIN, usecols=x_cols.keys(), dtype=x_cols)\nx_tst = pd.read_csv(TEST, usecols=x_cols.keys(), dtype=x_cols)\ny_trn = pd.read_csv(TARGET, usecols=y_cols.keys(), dtype=y_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PyTorch Wrappers\n\nBefore we can start training a `torch` model, we need to convert `pandas` data frames into PyTorch-specific data types. The main classes here are `Dataset` and `DataLoader`. The functions below split the dataset into training and validation subsets, and wrap them with data loaders."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_datasets(X, y, test_size=0.2, dropcols=ID_COLS, time_dim_first=False):\n    enc = LabelEncoder()\n    y_enc = enc.fit_transform(y)\n    X_grouped = create_grouped_array(X)\n    if time_dim_first:\n        X_grouped = X_grouped.transpose(0, 2, 1)\n    X_train, X_valid, y_train, y_valid = train_test_split(X_grouped, y_enc, test_size=0.1)\n    X_train, X_valid = [torch.tensor(arr, dtype=torch.float32) for arr in (X_train, X_valid)]\n    y_train, y_valid = [torch.tensor(arr, dtype=torch.long) for arr in (y_train, y_valid)]\n    train_ds = TensorDataset(X_train, y_train)\n    valid_ds = TensorDataset(X_valid, y_valid)\n    return train_ds, valid_ds, enc\n\n\ndef create_grouped_array(data, group_col='series_id', drop_cols=ID_COLS):\n    X_grouped = np.row_stack([\n        group.drop(columns=drop_cols).values[None]\n        for _, group in data.groupby(group_col)])\n    return X_grouped\n\n\ndef create_test_dataset(X, drop_cols=ID_COLS):\n    X_grouped = np.row_stack([\n        group.drop(columns=drop_cols).values[None]\n        for _, group in X.groupby('series_id')])\n    X_grouped = torch.tensor(X_grouped.transpose(0, 2, 1)).float()\n    y_fake = torch.tensor([0] * len(X_grouped)).long()\n    return TensorDataset(X_grouped, y_fake)\n\n\ndef create_loaders(train_ds, valid_ds, bs=512, jobs=0):\n    train_dl = DataLoader(train_ds, bs, shuffle=True, num_workers=jobs)\n    valid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=jobs)\n    return train_dl, valid_dl\n\n\ndef accuracy(output, target):\n    return (output.argmax(dim=1) == target).float().mean().item()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cyclic Learning Rate\n\nThe recent papers by L. Smith show us that the cyclic learning rate schedulers have very positive influence on model's convergence speed. In the following cells, we implement a simple cosine scheduler for our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"class CyclicLR(_LRScheduler):\n    \n    def __init__(self, optimizer, schedule, last_epoch=-1):\n        assert callable(schedule)\n        self.schedule = schedule\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cosine(t_max, eta_min=0):\n    \n    def scheduler(epoch, base_lr):\n        t = epoch % t_max\n        return eta_min + (base_lr - eta_min)*(1 + np.cos(np.pi*t/t_max))/2\n    \n    return scheduler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = 100\nsched = cosine(n)\nlrs = [sched(t, 1) for t in range(n * 4)]\nplt.plot(lrs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The LSTM Model\n\nOur classifier contains of several LSTM cells (hidden under the hood of `nn.LSTM`),  and one `nn.Linear` layer. Note that we use `batch_first=True` to make sure that the first dimension of our tensors is interpreted as a batch size, and the next one - as a time dimension."},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTMClassifier(nn.Module):\n    \"\"\"Very simple implementation of LSTM-based time-series classifier.\"\"\"\n    \n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.layer_dim = layer_dim\n        self.rnn = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.batch_size = None\n        self.hidden = None\n    \n    def forward(self, x):\n        h0, c0 = self.init_hidden(x)\n        out, (hn, cn) = self.rnn(x, (h0, c0))\n        out = self.fc(out[:, -1, :])\n        return out\n    \n    def init_hidden(self, x):\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n        return [t.cuda() for t in (h0, c0)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Preparing datasets')\ntrn_ds, val_ds, enc = create_datasets(x_trn, y_trn['surface'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bs = 128\nprint(f'Creating data loaders with batch size: {bs}')\ntrn_dl, val_dl = create_loaders(trn_ds, val_ds, bs, jobs=cpu_count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training Loop\n\nFinally, we are ready to bring everything together and train the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"input_dim = 10    \nhidden_dim = 256\nlayer_dim = 3\noutput_dim = 9\nseq_dim = 128\n\nlr = 0.0005\nn_epochs = 1000\niterations_per_epoch = len(trn_dl)\nbest_acc = 0\npatience, trials = 100, 0\n\nmodel = LSTMClassifier(input_dim, hidden_dim, layer_dim, output_dim)\nmodel = model.cuda()\ncriterion = nn.CrossEntropyLoss()\nopt = torch.optim.RMSprop(model.parameters(), lr=lr)\nsched = CyclicLR(opt, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/100))\n\nprint('Start model training')\n\nfor epoch in range(1, n_epochs + 1):\n    \n    for i, (x_batch, y_batch) in enumerate(trn_dl):\n        model.train()\n        x_batch = x_batch.cuda()\n        y_batch = y_batch.cuda()\n        sched.step()\n        opt.zero_grad()\n        out = model(x_batch)\n        loss = criterion(out, y_batch)\n        loss.backward()\n        opt.step()\n    \n    model.eval()\n    correct, total = 0, 0\n    for x_val, y_val in val_dl:\n        x_val, y_val = [t.cuda() for t in (x_val, y_val)]\n        out = model(x_val)\n        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n        total += y_val.size(0)\n        correct += (preds == y_val).sum().item()\n    \n    acc = correct / total\n\n    if epoch % 5 == 0:\n        print(f'Epoch: {epoch:3d}. Loss: {loss.item():.4f}. Acc.: {acc:2.2%}')\n\n    if acc > best_acc:\n        trials = 0\n        best_acc = acc\n        torch.save(model.state_dict(), 'best.pth')\n        print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n    else:\n        trials += 1\n        if trials >= patience:\n            print(f'Early stopping on epoch {epoch}')\n            break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The training is finished! Restoring the best model weights')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission\n\nNext, we load the best weights and run model on the testing dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_state_dict(torch.load('best.pth'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dl = DataLoader(create_test_dataset(x_tst), batch_size=64, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = []\nprint('Predicting on test dataset')\nfor batch, _ in test_dl:\n    batch = batch.permute(0, 2, 1)\n    out = model(batch.cuda())\n    y_hat = F.log_softmax(out, dim=1).argmax(dim=1)\n    test += y_hat.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = pd.read_csv(SAMPLE)\nsubmit['surface'] = enc.inverse_transform(test)\nsubmit.to_csv('submit.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}